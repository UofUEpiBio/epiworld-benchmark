---
format: gfm
---

# Using the CNN model to find the best parameters in SIR

Installing Packages if necessary:

```{r,echo=FALSE}
#| label: installing packages
library(epiworldR)
library(data.table)
library(tensorflow)
library(keras)
library(parallel)
library(keras3)
library(dplyr)
library(ggplot2)


```

## calling Preparation Function

Now we call the source to prepare the data we want to generate in the next step.

```{r}
#| label: calling preparation source
source("calibration/dataprep.R")
```

#### Generating parameters to Generate the Dataset

This function creates theta values which are needed parameters to generate the SIR dataset.

Preval, contact, recovery, and transmission rates are generated from distributions.

```{r}
#| label: generating theta
generate_theta <- function(N, n) {
  set.seed(1231)
  theta <- data.table(
    preval = sample((100:2000) / n, N, TRUE),
    crate  = rgamma(N, 5, 1),    # Mean 10
    ptran  = rbeta(N, 3, 7),     # Mean 3/(3 + 7) = 0.3
    prec   = rbeta(N, 10, 10*2 - 10) # Mean 10 / (10 * 2 - 10) = 0.5
  )
  return(theta)
}
```

### Generating the SIR Dataset

here a function simulates the SIR dataset with the EpiwolrdR package and with the values that are generated in the previous step.

```{r}
#| label: simulations
run_simulations <- function(N, n, ndays, ncores, theta, seeds) {
  matrices <- parallel::mclapply(1:N, FUN = function(i) {
    fn <- sprintf("calibration/simulated_data/sir-%06i.rds", i)
    
    if (file.exists(fn))
      return(readRDS(fn))
    
    set.seed(seeds[i])
    m <- theta[i, ModelSIRCONN(
      "mycon",
      prevalence        = preval,
      contact_rate      = crate,
      transmission_rate = ptran,
      recovery_rate     = prec, 
      n                 = n
    )]
    
    verbose_off(m)
    run(m, ndays = ndays)
    ans <- prepare_data(m)
    saveRDS(ans, fn)
    
    return(ans)
  }, mc.cores = ncores)
  
  return(matrices)
}

# Function to filter non-null matrices and update theta
filter_non_null <- function(matrices, theta) {
  is_not_null <- intersect(
    which(!sapply(matrices, inherits, what = "error")),
    which(!sapply(matrices, \(x) any(is.na(x))))
  )
  
  matrices <- matrices[is_not_null]
  theta    <- theta[is_not_null, ]
  
  return(list(matrices = matrices, theta = theta, N = length(is_not_null)))
}

```

# Section 1

### preparing the Dataset to be Ready for CNN

The function converts a list of matrices into a 3D array by extracting the first slice from each matrix and stacking them along a new dimension. This prepares the data for input into a TensorFlow model, which often requires data in a specific shape.

```{r}
#| label: prepare data for tensorflow
prepare_data_for_tensorflow <- function(matrices, N) {
  arrays_1d <- array(dim = c(N, dim(matrices[[1]][1,,])))
  
  for (i in seq_along(matrices)) {
    arrays_1d[i,,] <- matrices[[i]][1,,]
  }
  
  return(arrays_1d)
}


```

# Function to split data into training and test sets

```{r}
#| label: train and test 
split_data <- function(arrays_1d, theta2, N) {
  N_train <- floor(N * 0.7)
  id_train <- 1:N_train
  id_test <- (N_train + 1):N
  
  train <- list(
    x = array_reshape(arrays_1d[id_train,,], dim = c(N_train, dim(arrays_1d)[-1])),
    y = array_reshape(as.matrix(theta2)[id_train,], dim = c(N_train, ncol(theta2)))
  )
  
  test <- list(
    x = array_reshape(arrays_1d[id_test,,], dim = c(N - N_train, dim(arrays_1d)[-1])),
    y = array_reshape(as.matrix(theta2)[id_test,], dim = c(N - N_train, ncol(theta2)))
  )
  
  return(list(train = train, test = test))
}

```

# Building the CNN

This function constructs a CNN model using the `keras3` package in R. The model is intended for regression tasks (since it uses 'mse' loss and 'sigmoid' activation in the output layer) and consists of convolutional, pooling, flattening, and dense layers.

```{r}
#| label: building CNN model
build_cnn_model <- function(input_shape, output_units) {
  model <- keras3::keras_model_sequential() %>%
    keras3::layer_conv_2d(
      filters = 32,
      input_shape = c(input_shape, 1),
      activation = "linear",
      kernel_size = c(3, 5)
    ) %>%
    keras3::layer_max_pooling_2d(pool_size = 2, padding = 'same') %>%
    keras3::layer_flatten(input_shape = input_shape) %>%
    keras3::layer_dense(units = output_units, activation = 'sigmoid')
  
  model %>% compile(optimizer = 'adam', loss = 'mse', metric = 'accuracy')
  return(model)
}


```

step 1:

**Parameters**:

-   filters=32: The number of convolution filters (kernels) to use.

-   **``` input``_shape``= c(input_shape, 1) ```**: Specifies the shape of the input data, which `1` represents a single channel (e.g., grayscale images).

-   **`activation = "linear"`**: Uses the linear activation function (no activation is applied).

-   **`kernel_size = c(3, 5)`**: The dimensions of the convolution window (3 rows by 5 columns).

step 2

1.  **`layer_max_pooling_2d`**: Adds a max pooling layer to reduce the spatial dimensions of the output from the previous layer.

-   **Parameters**:

    -   **`pool_size = 2`**: The size of the pooling window (2x2).

    -   **`padding = 'same'`**: Pads the input so that the output has the same dimensions as the input.

    step 3

-   **`layer_flatten`**: Flattens the input into a one-dimensional vector.

-   **`input_shape = input_shape`**: Specifies the input shape for the flatten layer (may be redundant if input shape is already defined).

    step 4

-   **`layer_dense`**: Adds a fully connected layer to the model.

-   **Parameters**:

    -   **`units = output_units`**: The number of neurons in the layer, matching the number of output variables.

    -   **`activation = 'sigmoid'`**: Uses the sigmoid activation function, which outputs values between 0 and 1.

# Function to train the model

```{r}
#| label: train the model
train_model <- function(model, train_data, epochs = 100) {
  tensorflow::set_random_seed(331)
  model %>% fit(train_data$x, train_data$y, epochs = epochs, verbose = 2)
}


```

# Evaluation Function:

This function evaluates the trained model on the test data, makes predictions, calculates the Mean Absolute Error (MAE) for each output variable, and returns the predictions along with the MAE values.

```{r}
#| label: evaluation 
evaluate_model <- function(model, test_data, theta) {
  pred <- predict(model, x = test_data$x) |>
    as.data.table() |>
    setnames(colnames(theta))
  
  MAEs <- abs(pred - as.matrix(test_data$y)) |> colMeans() |> print()
  
  return(list(pred = pred, MAEs = MAEs))
}

```

# Function to plot the results

```{r}
#| label: plotting the results
plot_results <- function(pred, test_data, theta, MAEs, N, N_train) {
  # Prepare the data for plotting
  pred[, id := 1L:.N]
  pred[, crate := qlogis(crate) * 10]
  pred_long <- melt(pred, id.vars = "id")
  
  theta_long <- test_data$y |> as.data.table()
  setnames(theta_long, names(theta))
  theta_long[, id := 1L:.N]
  theta_long[, crate := qlogis(crate) * 10]
  theta_long <- melt(theta_long, id.vars = "id")
  
  alldat <- rbind(
    cbind(pred_long, Type = "Predicted"),
    cbind(theta_long, Type = "Observed")
  )
  
  # Plot 1: Boxplot of Predicted vs Observed values
  p1 <- ggplot(alldat, aes(x = value, colour = Type)) +
    facet_wrap(~variable, scales = "free") +
    geom_boxplot() +
    labs(title = "Boxplot: Predicted vs Observed")
  
  print(p1)  # Display the first plot
  
  # Prepare data for second plot
  alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
  
  vnames <- data.table(
    variable = c("preval", "crate", "ptran", "prec"),
    Name     = paste(
      c("Init. state", "Contact Rate", "P(transmit)", "P(recover)"),
      sprintf("(MAE: %.2f)", MAEs)
    )
  )
  
  alldat_wide <- merge(alldat_wide, vnames, by = "variable")
  
  # Plot 2: Observed vs Predicted with MAE labels
  p2 <- ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
    facet_wrap(~ Name, scales = "free") +
    geom_abline(slope = 1, intercept = 0) +
    geom_point(alpha = .2) +
    labs(
      title    = "Observed vs Predicted (validation set)",
      subtitle = sprintf(
        "The model includes %i simulated datasets, of which %i were used for training.",
        N, N_train
      ),
      caption  = "Predictions made using a CNN as implemented with loss function MAE."
    )
  
  print(p2)  # Display the second plot
}

# Call the function to show the plots
# plot_results(pred, test, theta, MAEs, N, N_train)


```

# The main function to orchestrate the entire process

```{r}
#| label: running one CNN model
main_pipeline <- function(N,n,ndays,ncores) {
  
  # Generate theta and seeds
  theta <- generate_theta(N, n)
  seeds <- sample.int(.Machine$integer.max, N, TRUE)
  
  # Run simulations
  matrices <- run_simulations(N, n, ndays, ncores, theta, seeds)
  
  # Filter non-null elements
  filtered_data <- filter_non_null(matrices, theta)
  matrices <- filtered_data$matrices
  theta <- filtered_data$theta
  N <- filtered_data$N
  
  # Prepare data for TensorFlow
  arrays_1d <- prepare_data_for_tensorflow(matrices, N)
  
  # Save theta and simulations data
  theta2 <- copy(theta)
  theta2[, crate := plogis(crate / 10)]
  saveRDS(list(theta = theta2, simulations = arrays_1d), file = "calibration/sir.rds", compress = TRUE)
  
  # Split data into training and testing sets
  data_split <- split_data(arrays_1d, theta2, N)
  train <- data_split$train
  test <- data_split$test
  
  # Build and train the CNN model
  model <- build_cnn_model(dim(arrays_1d)[-1], ncol(theta2))
  train_model(model, train)
  
  # Evaluate the model
  eval_results <- evaluate_model(model, test, theta)
  pred <- eval_results$pred
  MAEs <- eval_results$MAEs
  
  # Plot the results
  plot_results(pred, test, theta, MAEs, N, floor(N * 0.7))
}

```

The `main_pipeline` function automates the process of:

-   **Data Generation**: Creates synthetic parameters and seeds for simulation.

-   **Simulation Execution**: Runs simulations to generate data.

-   **Data Preprocessing**: Filters and formats data for modeling.

-   **Model Training**: Builds and trains a CNN model on the data.

-   **Model Evaluation**: Assesses model performance using test data.

-   **Result Visualization**: Provides graphical insights into model predictions and errors.

# Run the whole process with given values

```{r}
#| label: give parameters and run
N <- 2e4
n <- 5000
ndays <- 50
ncores <- 20
# Execute the main pipeline
main_pipeline(N,n,ndays,ncores)

```

# Section 2

Now we can Run a CNN model to find the best parameters we can use for our CNN model to perform

```{r}
build_cnn_model <- function(input_shape, output_units, filters, kernel_size, activation, dense_units) {
  model <- keras3::keras_model_sequential() %>%
    keras3::layer_conv_2d(
      filters = filters,                      # Use dynamic filters
      kernel_size = kernel_size,              # Use dynamic kernel size
      activation = activation,                # Use dynamic activation
      input_shape = c(input_shape, 1)         # Assuming single-channel input (e.g., grayscale)
    ) %>%
    keras3::layer_max_pooling_2d(pool_size = 2, padding = 'same') %>%
    keras3::layer_flatten() %>%
    keras3::layer_dense(units = dense_units, activation = activation) %>%
    keras3::layer_dense(units = output_units, activation = 'sigmoid')  # Output layer

  model %>% compile(
    optimizer = 'adam', 
    loss = 'mse', 
    metrics = 'accuracy'
  )

  return(model)
}

```

```{r}
#|label: main_pipeline for tuning
main_pipeline <- function(N, n, ndays, ncores) {
  
  # Generate theta and seeds
  theta <- generate_theta(N, n)
  seeds <- sample.int(.Machine$integer.max, N, TRUE)
  
  # Run simulations
  matrices <- run_simulations(N, n, ndays, ncores, theta, seeds)
  
  # Filter non-null elements
  filtered_data <- filter_non_null(matrices, theta)
  matrices <- filtered_data$matrices
  theta <- filtered_data$theta
  N <- filtered_data$N
  
  # Prepare data for TensorFlow
  arrays_1d <- prepare_data_for_tensorflow(matrices, N)
  
  # Save theta and simulations data
  theta2 <- copy(theta)
  theta2[, crate := plogis(crate / 10)]  # Apply logit transform to crate
  saveRDS(list(theta = theta2, simulations = arrays_1d), file = "calibration/sir.rds", compress = TRUE)
  
  # Split data into training and testing sets
  data_split <- split_data(arrays_1d, theta2, N)
  train <- data_split$train
  test <- data_split$test
  
  # Return train and test sets for future use
  return(list(train = train, test = test, theta2 = theta2, arrays_1d = arrays_1d))
}

```

The `main_pipeline` function streamlines the process of generating and preparing data for machine learning models. By returning the prepared datasets, it allows for modularity and flexibility, enabling you to focus on building and evaluating models without worrying about data preparation each time.

# Function to experiment with different CNN configurations and track performance

The `experiment_cnn_models` function systematically experiments with different configurations (hyperparameters) of a Convolutional Neural Network (CNN) to determine which combination yields the best performance, measured by the Mean Absolute Error (MAE) on the test dataset.

```{r}
#| label: find different models with different parameters
experiment_cnn_models <- function(train_data, test_data, theta, input_shape, output_units, epochs = 10) {
  # Define the grid of hyperparameters to explore
  filter_sizes <- c(16, 32, 64)         # Number of filters
  kernel_sizes <- list(c(3, 3), c(5, 5), c(3,5)) # Kernel sizes
  activations <- c("relu", "linear")     # Activation functions
  dense_units <- c(32, 64, 128)          # Dense layer units
  
  # Store the results in a data frame for comparison
  results <- data.table(
    filters = integer(),
    kernel_size = character(),
    activation = character(),
    dense_units = integer(),
    MAE = numeric()
  )
  
  # Loop over all combinations of hyperparameters
  for (filters in filter_sizes) {
    for (kernel_size in kernel_sizes) {
      for (activation in activations) {
        for (dense in dense_units) {
          cat(sprintf("\nTraining model with filters=%d, kernel_size=%s, activation=%s, dense_units=%d\n",
                      filters, paste(kernel_size, collapse="x"), activation, dense))
          
          # Build the model with current hyperparameters
          model <- build_cnn_model(
            input_shape = input_shape, 
            output_units = output_units,
            filters = filters, 
            kernel_size = kernel_size, 
            activation = activation, 
            dense_units = dense
          )
          
          # Train the model
          train_model(model, train_data, epochs = epochs)
          
          # Evaluate the model
          eval_results <- evaluate_model(model, test_data, theta)
          pred <- eval_results$pred
          MAEs <- eval_results$MAEs
          
          # Store the configuration and the MAE result
          results <- rbind(results, data.table(
            filters = filters,
            kernel_size = paste(kernel_size, collapse = "x"),
            activation = activation,
            dense_units = dense,
            MAE = mean(MAEs)  # Storing the mean MAE for comparison
          ))
          
          # Output the performance
          cat("Mean MAE for this configuration:", mean(MAEs), "\n")
        }
      }
    }
  }
  
  # Return the results data table with all configurations and their performance
  return(results)
}


```

**Function Breakdown**:

1.  **Hyperparameter Grid Definition**:

    -   **`filter_sizes`**: Specifies different numbers of filters to try (16, 32, 64).

    -   **`kernel_sizes`**: Lists different kernel size combinations to test (3x3, 5x5, 3x5).

    -   **`activations`**: Includes activation functions to experiment with ("relu", "linear").

    -   **`dense_units`**: Defines different sizes for the dense layer (32, 64, 128).

2.  **Results Initialization**:

    -   Creates an empty `data.table` called `results` to store the performance metrics of each model configuration.

3.  **Hyperparameter Combination Loop**:

    -   Nested loops iterate over every combination of the hyperparameters.

    -   For each combination:

        -   **Model Training**:

            -   Prints the current configuration being trained.

            -   Builds the CNN model using the `build_cnn_model` function with the current hyperparameters.

            -   Trains the model on the provided training data using `train_model`.

        -   **Model Evaluation**:

            -   Evaluates the model on the test data using `evaluate_model`.

            -   Calculates the mean MAE from the evaluation results.

        -   **Results Recording**:

            -   Appends the current hyperparameter settings and the corresponding mean MAE to the `results` table.

            -   Prints the mean MAE for the current configuration.

4.  **Return Statement**:

    -   After all combinations have been evaluated, the function returns the `results` table containing all configurations and their mean MAE scores.

# Running the Process to Find the Best Model

```{r,epochs=FALSE}
#| label: finding best model
N <- 2e4
n <- 5000
ndays <- 50
ncores <- 20

# Prepare the dataset
pipeline_data <- main_pipeline(N, n, ndays, ncores)
train <- pipeline_data$train
test <- pipeline_data$test
theta2 <- pipeline_data$theta2
arrays_1d <- pipeline_data$arrays_1d

# Define the input shape and number of output units
input_shape <- dim(arrays_1d)[-1]  # Shape excluding the batch size
output_units <- ncol(theta2)

# Run the CNN model experiments
results <- experiment_cnn_models(train, test, theta2, input_shape, output_units)
```

```{r}
# Display the results of all configurations and their MAEs
print(results)
```

```{r,epochs=FALSE}
#| label: find best parameters and run the model
# Find the best model configuration
find_best_model <- function(results) {
  # Select the row with the minimum MAE
  best_model <- results[which.min(MAE)]
  return(best_model)
}

# Train and evaluate the best model
train_and_evaluate_best_model <- function(best_model, train_data, test_data, theta, input_shape, output_units, epochs = 10) {
  # Extract the best hyperparameters
  best_filters <- best_model$filters
  best_kernel_size <- as.integer(unlist(strsplit(best_model$kernel_size, "x")))
  best_activation <- best_model$activation
  best_dense_units <- best_model$dense_units
  
  # Build the best model
  model <- build_cnn_model(
    input_shape = input_shape,
    output_units = output_units,
    filters = best_filters,
    kernel_size = best_kernel_size,
    activation = best_activation,
    dense_units = best_dense_units
  )
  
  # Train the best model
  train_model(model, train_data, epochs = epochs)
  
  # Evaluate the best model
  eval_results <- evaluate_model(model, test_data, theta)
  return(list(model = model, eval_results = eval_results))
}

```

```{r}
#| label: visualize the best model
plot_results <- function(pred, test_data, theta, MAEs, N, N_train) {
  # Prepare the data for plotting
  pred[, id := 1L:.N]
  pred[, crate := qlogis(crate) * 10]
  pred_long <- melt(pred, id.vars = "id")
  
  theta_long <- test_data$y |> as.data.table()
  setnames(theta_long, names(theta))
  theta_long[, id := 1L:.N]
  theta_long[, crate := qlogis(crate) * 10]
  theta_long <- melt(theta_long, id.vars = "id")
  
  alldat <- rbind(
    cbind(pred_long, Type = "Predicted"),
    cbind(theta_long, Type = "Observed")
  )
  
  # Plot 1: Boxplot of Predicted vs Observed values
  p1 <- ggplot(alldat, aes(x = value, colour = Type)) +
    facet_wrap(~variable, scales = "free") +
    geom_boxplot() +
    labs(title = "Boxplot: Predicted vs Observed")
  
  print(p1)  # Display the first plot
  
  # Prepare data for second plot
  alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
  
  vnames <- data.table(
    variable = c("preval", "crate", "ptran", "prec"),
    Name     = paste(
      c("Init. state", "Contact Rate", "P(transmit)", "P(recover)"),
      sprintf("(MAE: %.2f)", MAEs)
    )
  )
  
  alldat_wide <- merge(alldat_wide, vnames, by = "variable")
  
  # Plot 2: Observed vs Predicted with MAE labels
  p2 <- ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
    facet_wrap(~ Name, scales = "free") +
    geom_abline(slope = 1, intercept = 0) +
    geom_point(alpha = .2) +
    labs(
      title    = "Observed vs Predicted (validation set)",
      subtitle = sprintf(
        "The model includes %i simulated datasets, of which %i were used for training.",
        N, N_train
      ),
      caption  = "Predictions made using a CNN as implemented with loss function MAE."
    )
  
  print(p2)  # Display the second plot
}
```

```{r}
#| label: print best model
best_model <- find_best_model(results)

print(best_model)
```

```{r,epochs=FALSE}
#| label: learn best model
best_model_results <- train_and_evaluate_best_model(best_model, train, test, theta2, input_shape, output_units)


```

```{r}
#| label: print MAE for the best model
pred <- best_model_results$eval_results$pred
MAEs <- best_model_results$eval_results$MAEs
print(MAEs)
```

```{r}
#| label: plotting the best results
plot_results(pred, test, theta2, MAEs, N, floor(N * 0.7))

```
