---
format: gfm
---

# Using the LSTM model to Find the Best Parameters Used in Generating the SIR Model

# Section 1:

#### implementing LSTM model

installing required packages:

```{r}
#| label: installing packages
library(epiworldR)
library(data.table)
library(tensorflow)
library(keras)
library(parallel)
library(keras3)
library(dplyr)
library(ggplot2)



```

## calling Preparation Function

Now we call the source to prepare the data we want to generate in the next step.

```{r}
#| label: sourcing-functions
# devtools::install_github("UofUEpi/epiworldR")
source("calibration/dataprep.R")
```

# Set parameters

```{r}
#| label: parameters

N     <- 2e4
n     <- 5000
ndays <- 50
ncores <- 20
```

# Generate parameters (theta)

The `generate_simulation_parameters` function generates a table of parameters (`theta`) for the SIR model simulations.

```{r}
#| label: generate theta
set.seed(1231)
generate_simulation_parameters <- function(N, n) {
  theta <- data.table(
    preval = sample((100:2000) / n, N, TRUE),
    crate  = rgamma(N, 5, 1),  # Mean 10
    ptran  = rbeta(N, 3, 7),   # Mean 3/(3 + 7) = 0.3
    prec   = rbeta(N, 10, 10 * 2 - 10)  # Mean 10 / (10 * 2 - 10) = .5
  )
  theta[, hist(crate)]
  return(theta)
}
```

-   **Inputs**:

    -   `N`: The number of simulations or rows of parameter sets to generate.

    -   `n`: The population size used for prevalence calculations.

-   **Outputs**:

    -   The function returns a `data.table` containing four key epidemiological parameters:

        1.  **`preval`**: Prevalence, or the initial proportion of the population that is infected. It is sampled from a range between `100/n` and `2000/n` (which corresponds to infection rates between 0.02% and 0.4%).

        2.  **`crate`**: Contact rate, which is the rate at which individuals in the population come into contact with others. It is generated from a Gamma distribution with a shape parameter of 5 and a rate parameter of 1 (which gives a mean contact rate of 5).

        3.  **`ptran`**: Transmission probability, or the likelihood of disease transmission upon contact, drawn from a Beta distribution with shape parameters 3 and 7. This gives a mean transmission rate of 0.3 (since 3 / (3 + 7) = 0.3).

        4.  **`prec`**: Recovery probability, or the likelihood of recovery after infection. It is drawn from a Beta distribution with shape parameters of 10 and 10, giving a mean recovery rate of 0.5 (since 10 / (10 \* 2 - 10) = 0.5).

# run epidemic model simulations in parallel

`run_epidemic_simulations` Function runs SIR (Susceptible-Infectious-Recovered) model simulations in parallel for `N` iterations, using a set of parameters (`theta`) for each simulation.

```{r}
#| label: run simulations
run_epidemic_simulations <- function(N, theta, seeds, n, ndays, ncores) {
  matrices <- parallel::mclapply(1:N, FUN = function(i) {
    fn <- sprintf("calibration/simulated_data/sir-%06i.rds", i)
    if (file.exists(fn)) return(readRDS(fn))
    set.seed(seeds[i])
    m <- theta[i, ModelSIRCONN("mycon", prevalence = preval, contact_rate = crate, transmission_rate = ptran, recovery_rate = prec, n = n)]
    verbose_off(m)
    run(m, ndays = ndays)
    ans <- prepare_data(m)
    saveRDS(ans, fn)
    return(ans)
  }, mc.cores = ncores)
  return(matrices)
}
```

```{r}
#| label: filtering the data
filter_valid_simulations <- function(matrices, theta) {
  valid_indices <- intersect(
    which(!sapply(matrices, inherits, what = "error")),
    which(!sapply(matrices, \(x) any(is.na(x))))
  )
  matrices <- matrices[valid_indices]
  theta <- theta[valid_indices, ]
  return(list(matrices = matrices, theta = theta, N = length(valid_indices)))
}
```

**Inputs**:

-   `N`: Number of simulations to run.

-   `theta`: A table containing the parameters for each simulation (prevalence, contact rate, transmission rate, and recovery rate).

-   `seeds`: A set of random seeds to ensure each simulation has different random behavior.

-   `n`: Population size for the simulation.

-   `ndays`: Number of days to simulate.

-   `ncores`: Number of CPU cores to use for parallel processing.

# prepare simulation data for neural network training

`prepare_neural_network_data` Function prepares the simulation data and parameters for use in training a neural network.

```{r}
#| label: preparing data for tensorflow
# Function to prepare simulation data for neural network training
prepare_neural_network_data <- function(N, matrices, theta) {
  arrays_1d <- array(dim = c(N, dim(matrices[[1]][1, , ])))
  for (i in seq_along(matrices)) {
    arrays_1d[i, , ] <- matrices[[i]][1, , ]
  }
  theta2 <- copy(theta)
  theta2[, crate := plogis(crate / 10)]
  return(list(arrays_1d = arrays_1d, theta2 = theta2))
}

# Function to save the prepared data to an RDS file
save_prepared_data <- function(theta2, arrays_1d) {
  saveRDS(
    list(
      theta = theta2,
      simulations = arrays_1d
    ),
    file = "calibration/sir.rds",
    compress = TRUE
  )
}
```

# Split data into training and test sets

```{r}
#| label: train and test data
split_train_test_data <- function(N, arrays_1d, theta2) {
  N_train <- floor(N * 0.7)
  id_train <- 1:N_train
  id_test <- (N_train + 1):N
  train <- list(
    x = array_reshape(arrays_1d[id_train, , ], dim = c(N_train, dim(arrays_1d)[-1])),
    y = array_reshape(as.matrix(theta2)[id_train, ], dim = c(N_train, ncol(theta2)))
  )
  test <- list(
    x = array_reshape(arrays_1d[id_test, , ], dim = c(N - N_train, dim(arrays_1d)[-1])),
    y = array_reshape(as.matrix(theta2)[id_test, ], dim = c(N - N_train, ncol(theta2)))
  )
  return(list(train = train, test = test))
}
```

-   **Inputs**:

    -   `N`: The number of valid simulations.

    -   `matrices`: A list of simulation results (from `run_epidemic_simulations`).

    -   `theta`: A table of parameters used for the simulations.

-   **Functionality**:

    -   **Creating an Array for Simulations**:

        -   `arrays_1d`: A 3D array is created to store the first slice of data (e.g., infections) from each simulation result. The dimensions are `(N, rows, cols)` where `N` is the number of simulations.

        -   Each simulation's result is extracted (`matrices[[i]][1, , ]`) and added to `arrays_1d`.

    -   **Adjusting Parameters**:

        -   `theta2`: A copy of `theta` is created, and the contact rate (`crate`) is transformed using the logistic function (`plogis(crate / 10)`) to map values into a probability range (0, 1).

# Build and train the LSTM model

`build_and_train_lstm` Function builds and trains an LSTM (Long Short-Term Memory) neural network model using the `keras3` package.

```{r}
#| label: build LSTM model
build_and_train_lstm <- function(train, theta2, arrays_1d) {
  model <- keras3::keras_model_sequential() %>%
    keras3::layer_lstm(units = 64, input_shape = c(dim(arrays_1d)[2], dim(arrays_1d)[3]), return_sequences = FALSE) %>%
    keras3::layer_dense(units = ncol(theta2), activation = 'sigmoid')
  
  model %>% compile(optimizer = 'adam', loss = 'mse', metrics = 'accuracy')
  
  tensorflow::set_random_seed(331)
  model %>% fit(train$x, train$y, epochs = 100, verbose = 0)
  
  return(model)
}
```

**Inputs**:

-   `train`: The training dataset, consisting of `train$x` (input data) and `train$y` (target parameters).

-   `theta2`: The adjusted table of parameters that the model will learn to predict.

-   `arrays_1d`: The 3D array containing simulation data, which helps define the input shape of the LSTM.

-   `lstm_units`: The number of LSTM units (default is 64) in the hidden layer.

-   `epochs`: Number of training epochs (default is 100), i.e., how many times the model will iterate over the training data.

-   `loss_function`: The loss function used to optimize the model (default is Mean Squared Error, `'mse'`).

-   `optimizer`: The optimizer for training (default is Adam).

-   `activation`: The activation function used in the output layer (default is `'sigmoid'`).

# predict using the trained model

```{r}
#| label: predictions and mae 
# Function to predict using the trained model
make_predictions <- function(model, test, theta) {
  pred <- predict(model, x = test$x) |> as.data.table() |> setnames(colnames(theta))
  return(pred)
}

# Function to compute the Mean Absolute Error (MAE)
compute_mae <- function(pred, test) {
  MAEs <- abs(pred - as.matrix(test$y)) |> colMeans() |> print()
  return(MAEs)
}
```

# Visualize model predictions against observed data

```{r}
#| label: visualize the model
visualize_predictions <- function(pred, test, MAEs, theta, N, N_train) {
  pred[, id := 1L:.N]
  pred[, crate := qlogis(crate) * 10]
  pred_long <- melt(pred, id.vars = "id")
  
  theta_long <- test$y |> as.data.table()
  setnames(theta_long, names(theta))
  theta_long[, id := 1L:.N]
  theta_long[, crate := qlogis(crate) * 10]
  theta_long <- melt(theta_long, id.vars = "id")
  
  alldat <- rbind(cbind(pred_long, Type = "Predicted"), cbind(theta_long, Type = "Observed"))
  
  ggplot(alldat, aes(x = value, colour = Type)) +
    facet_wrap(~variable, scales = "free") +
    geom_boxplot()
  
  alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
  
  vnames <- data.table(
    variable = c("preval", "crate", "ptran", "prec"),
    Name     = paste(c("Init. state", "Contact Rate", "P(transmit)", "P(recover)"), sprintf("(MAE: %.2f)", MAEs))
  )
  
  alldat_wide <- merge(alldat_wide, vnames, by = "variable")
  
  ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
    facet_wrap(~ Name, scales = "free") +
    geom_abline(slope = 1, intercept = 0) +
    geom_point(alpha = .2) +
    labs(
      title = "Observed vs Predicted (validation set)",
      subtitle = sprintf("The model includes %i simulated datasets, of which %i were used for training.", N, N_train),
      caption = "Predictions made using a CNN as implemented with loss function MAE."
    )
}
```

# Main execution block

```{r}
#| label: run the single LSTM model
theta <- generate_simulation_parameters(N, n)
seeds <- sample.int(.Machine$integer.max, N, TRUE)
matrices <- run_epidemic_simulations(N, theta, seeds, n, ndays, ncores)

# Filter and prepare data
valid_data <- filter_valid_simulations(matrices, theta)
N <- valid_data$N
theta <- valid_data$theta
matrices <- valid_data$matrices

# Prepare for neural network
nn_data <- prepare_neural_network_data(N, matrices, theta)
arrays_1d <- nn_data$arrays_1d
theta2 <- nn_data$theta2

# Save data
save_prepared_data(theta2, arrays_1d)

# Split train and test data
train_test_data <- split_train_test_data(N, arrays_1d, theta2)
train <- train_test_data$train
test <- train_test_data$test
```

```{r,epochs=FALSE}
#| label: run the model
# Build, train, and evaluate the model
model <- build_and_train_lstm(train, theta2, arrays_1d)
```

```{r}
#| label: prediction
pred <- make_predictions(model, test, theta)
```

```{r}
#| label: MAE
MAEs <- compute_mae(pred, test)
print(MAEs)
```

```{r}
#| label: Visualize the results
visualize_predictions(pred, test, MAEs, theta, N, floor(N * 0.7))

```

# Section 2:

# Finding the Best Model

Now we want to find the best model for LSTM.

# build and train the LSTM model with hyperparameters

The `build_and_train_model` function builds, trains, and evaluates an LSTM (Long Short-Term Memory) neural network using the `keras3` library in R for time-series or sequential data.

```{r}
#|label: Function to build and train the LSTM model with hyperparameters
build_and_train_model <- function(train, test, theta, N_train, ndays, seed,
                                  units, optimizer, loss, epochs, verbose = 0) {
  # Build the LSTM model
  model <- keras_model_sequential() %>%
    layer_lstm(
      units = units,
      input_shape = c(ndays, 1)
    ) %>%
    layer_dense(
      units = ncol(theta),
      activation = 'sigmoid'  # Fixed activation for dense layer
    )
  
  # Compile the model
  model %>% compile(
    optimizer = optimizer,
    loss      = loss,
    metrics   = 'mae'
  )
  
  # Set random seed
  tensorflow::set_random_seed(seed)
  
  # Fit the model
  history <- model %>% fit(
    x = train$x,
    y = train$y,
    epochs = epochs,
    verbose = verbose
  )
  
  # Make predictions
  pred <- predict(model, x = test$x) %>%
    as.data.table() %>%
    setnames(colnames(theta))
  
  # Calculate MAEs
  MAEs <- abs(pred - test$y) %>%
    colMeans()
  
  # Return the MAEs and predictions
  list(pred = pred, MAEs = MAEs, model = model)
}
```

### **Building the LSTM Model**

-   The function creates a sequential model using `keras3::keras_model_sequential()`.

-   An LSTM layer (`keras3::layer_lstm()`) is added with customizable parameters:

    -   `units`: the number of LSTM units (neurons) in the layer.

    -   `activation_lstm`: the activation function applied within the LSTM units.

    -   `input_shape`: the shape of the input data, which is defined as the number of time steps (`dim(train$x)[2]`) and features (`dim(train$x)[3]`).

    -   `return_sequences = FALSE`: this flag means that only the final LSTM output (not a sequence) is passed to the next layer.

-   A fully connected (dense) layer (`keras3::layer_dense()`) is added with:

    -   `units = ncol(theta)`: the number of output neurons, which matches the number of columns in the `theta` matrix.

    -   `activation_dense`: the activation function applied in the dense layer (e.g., 'sigmoid' or 'linear').

# visualize results

```{r}
#| label: Function to visualize results
#|label: Function to visualize results
visualize_results <- function(pred, test, theta, MAEs, N, N_train, output_file = NULL) {
  pred[, id := 1L:.N]
  pred_long <- melt(pred, id.vars = "id")
  
  theta_long <- as.data.table(test$y)
  setnames(theta_long, names(theta))
  theta_long[, id := 1L:.N]
  theta_long <- melt(theta_long, id.vars = "id")
  
  alldat <- rbind(
    cbind(pred_long, Type = "Predicted"),
    cbind(theta_long, Type = "Observed")
  )
  
  # Density plots
  p1 <- ggplot(alldat, aes(x = value, colour = Type)) +
    facet_wrap(~variable, scales = "free") +
    geom_density() +
    labs(title = "Density Plots of Predicted vs Observed Values",
         subtitle = "Comparing distributions of predicted and observed parameters",
         x = "Parameter Value", y = "Density")
  
  print(p1)
  
  # Scatter plots of Observed vs Predicted
  alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
  
  vnames <- data.table(
    variable = names(theta),
    Name     = paste(
      c("Initial Prevalence", "Contact Rate", "Transmission Probability", "Recovery Probability"),
      sprintf("(MAE: %.4f)", MAEs)
    )
  )
  
  alldat_wide <- merge(alldat_wide, vnames, by = "variable")
  
  p2 <- ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
    facet_wrap(~ Name, scales = "free") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
    geom_point(alpha = .2) +
    labs(
      title    = "Observed vs Predicted (Test Set)",
      subtitle = sprintf(
        "Best Model with Mean MAE: %.4f",
        mean(MAEs)
      ),
      x = "Observed Values",
      y = "Predicted Values"
    )
  
  print(p2)
}

#| label: simulate_data
simulate_data <- function(N = 20000, n = 5000, ndays = 50, ncores = 20) {
  # Generate theta parameters (example based on earlier parts)
  theta <- generate_simulation_parameters(N, n)
  seeds <- sample.int(.Machine$integer.max, N, TRUE)
  
  # Run epidemic simulations (using SIR model)
  matrices <- run_epidemic_simulations(N, theta, seeds, n, ndays, ncores)
  
  # Filter valid simulations
  valid_data <- filter_valid_simulations(matrices, theta)
  
  # Prepare neural network input data
  nn_data <- prepare_neural_network_data(valid_data$N, valid_data$matrices, valid_data$theta)
  
  # Save the prepared data (optional)
  save_prepared_data(nn_data$theta2, nn_data$arrays_1d)
  
  # Return the prepared datasets
  return(list(train_test_data = nn_data, N = valid_data$N, theta = valid_data$theta))
}
#| label: prepare_data_sets
prepare_data_sets <- function(N, arrays_1d, theta2) {
  # Split the data into training and test sets
  data_splits <- split_train_test_data(N, arrays_1d, theta2)
  
  # Return the split datasets
  return(data_splits)
}


```

### 1. **Parameter Generation**

-   `generate_simulation_parameters(N, n)`: This function generates simulation parameters (`theta`) for the epidemic simulations based on the input parameters `N` and `n`. The exact nature of `theta` depends on the simulation context.

-   `sample.int(.Machine$integer.max, N, TRUE)`: This generates a set of random seeds for reproducibility across `N` simulations.

### 2. **Run Simulations**

-   `run_epidemic_simulations(N, theta, seeds, n, ndays, ncores)`: This runs the epidemic simulations using the parameters (`theta`) and seeds across `ncores` processing cores. The results are stored in `matrices`, which likely contain simulated data for different scenarios.

### 3. **Filter and Prepare Data**

-   `filter_valid_simulations(matrices, theta)`: This function filters out invalid or null simulations. It adjusts `matrices`, `theta`, and `N` to ensure only valid simulations are used for further processing.

-   `prepare_neural_network_data(N, matrices, theta)`: Prepares the data for input into the LSTM neural network, converting the simulation results (`matrices`) and parameters (`theta`) into appropriate arrays (`arrays_1d`) for TensorFlow.

### 4. **Save Prepared Data**

-   `save_prepared_data(theta2, arrays_1d)`: Saves the prepared data (optional), possibly as `.rds` or some other serialized format, allowing you to reload it later.

### 5. **Split Train and Test Data**

-   `split_train_test_data(N, arrays_1d, theta2)`: Splits the dataset into training and test sets. `train_test_data` includes `train`, `test`, and the number of training samples (`N_train`).

### 6. **Define Hyperparameter Grid**

-   `expand.grid()`: Creates a grid of hyperparameters for tuning. The parameters include:

    -   `units`: Number of LSTM units (50 or 64).

    -   `activation_lstm`: Activation function for LSTM (e.g., `'relu'`).

    -   `activation_dense`: Activation function for the dense output layer (`'sigmoid'` or `'linear'`).

    -   `optimizer`: Optimizer used for training (`'adam'`).

    -   `loss`: Loss function for training (`'mse'` or `'mae'`).

    -   `epochs`: Number of training epochs (20).

    -   `batch_size`: Batch size for training (32 or 64).

This grid allows for different combinations of hyperparameters to be tested and tuned for the best model performance.

### 7. **Hyperparameter Tuning Loop**

-   The loop runs through all possible hyperparameter combinations:

    -   Extracts the current hyperparameters from the grid (`units`, `activation_lstm`, `activation_dense`, `optimizer`, `loss`, `epochs`, `batch_size`).

    -   Sets a reproducible seed (`seed = 331`).

    -   Calls the `build_and_train_model` function to build, train, and test the model for each combination of hyperparameters.

### 8. **Error Handling**

-   A `tryCatch` block is used to handle errors during model building or training. If an error occurs, it moves to the next combination of hyperparameters without stopping the entire process.

### 9. **Evaluating the Models**

-   After each model is trained, the mean absolute errors (MAEs) are calculated.

-   The average MAE is stored for each hyperparameter configuration.

-   If the current model has the best MAE (lowest), it updates the stored `best_model`, `best_pred`, `best_MAEs`, and `best_params`.

### 10. **Results**

-   After testing all hyperparameter combinations, the function prints the best model's hyperparameters and the lowest MAE achieved.

### 11. **Visualization**

-   `visualize_results()`: This function visualizes the results, such as comparing predicted values (`best_pred`) to the actual values (`test$y`). It likely includes error analysis, plotting, and model diagnostics.

# Run the main function

```{r}
#| label: main-execution-function
#| label: main-execution-function
main <- function(N = 20000, n = 5000, ndays = 50, ncores = 20) {
  # Simulate data
  simulated_data <- simulate_data(N, n, ndays, ncores)
  
  # Extract the relevant data
  train_test_data <- prepare_data_sets(simulated_data$N, simulated_data$train_test_data$arrays_1d, simulated_data$train_test_data$theta2)
  
  train <- train_test_data$train
  test <- train_test_data$test
  theta <- simulated_data$theta
  N <- simulated_data$N
  N_train <- length(train$x)
  
  # Define hyperparameter grid
  hyper_grid <- expand.grid(
    units = c(50, 64, 30),
    optimizer = c('adam'),
    loss = c('mae'),
    epochs = c(20),
    stringsAsFactors = FALSE
  )
  
  # Initialize variables to store the best model
  best_MAE <- Inf
  best_model <- NULL
  best_pred <- NULL
  best_MAEs <- NULL
  best_params <- NULL
  
  # Loop over hyperparameter combinations
  for (i in 1:nrow(hyper_grid)) {
    cat("Testing model", i, "of", nrow(hyper_grid), "\n")
    
    # Extract hyperparameters
    units <- hyper_grid$units[i]
    optimizer <- hyper_grid$optimizer[i]
    loss <- hyper_grid$loss[i]
    epochs <- hyper_grid$epochs[i]
    
    # Set a seed for reproducibility
    seed <- 331
    
    # Build and train the model
    model_results <- tryCatch(
      {
        build_and_train_model(
          train = train,
          test = test,
          theta = theta,
          N_train = N_train,
          ndays = ndays,
          seed = seed,
          units = units,
          optimizer = optimizer,
          loss = loss,
          epochs = epochs,
          verbose = 0
        )
      },
      error = function(e) {
        cat("Error in model", i, ":", e$message, "\n")
        return(NULL)
      }
    )
    
    # If the model failed, skip to the next iteration
    if (is.null(model_results)) next
    
    # Get the MAEs
    MAEs <- model_results$MAEs
    
    # Store the average MAE
    hyper_grid$MAE[i] <- mean(MAEs)
    
    # If this is the best MAE so far, save the model and predictions
    if (hyper_grid$MAE[i] < best_MAE) {
      best_MAE <- hyper_grid$MAE[i]
      best_model <- model_results$model
      best_pred <- model_results$pred
      best_MAEs <- MAEs
      best_params <- hyper_grid[i,]
    }
  }
  
  # Print the best hyperparameters
  cat("Best model parameters:\n")
  print(best_params)
  cat("Best MAE:", best_MAE, "\n")
  
  # Visualize results
  print(visualize_results(best_pred, test, theta, best_MAEs, N, N_train))
}


```

```{r,echo=TRUE}
#|label: Run the main function
N <- 2e4   # Adjust N as needed
n <- 5000
ndays <- 50
ncores <- 20
main(N, n, ndays, ncores)
```
