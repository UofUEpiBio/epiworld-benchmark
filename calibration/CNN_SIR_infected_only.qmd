---
title: "Implementing the CNN Model Just by Knowing the Infections Count to Find the Closest Parameters to Simulate SIR Models"
format: gfm
editor: visual
---

Load required libraries:

```{r,epochs=FALSE}

#| label: Load required libraries
library(data.table)
library(parallel)
library(keras)
library(ggplot2)
library(reshape2)
library(tensorflow)
library(keras3)
```

# simulate data

`imulate_data` function performs a simulation for epidemiological modeling using the SIR (Susceptible-Infectious-Recovered) model across multiple iterations, to generate and save data for further analysis.

```{r}
#|label: Function to simulate data
simulate_data <- function(N = 2e4, n = 5000, ndays = 50, ncores = 20, seed = 1231, savefile = "calibration/sir.rds") {
  source("calibration/dataprep.R")
  
  set.seed(seed)
  
  theta <- data.table(
    preval = sample((100:2000)/n, N, TRUE),
    crate  = rgamma(N, 5, 1),    # Mean 5
    ptran  = rbeta(N, 3, 7),         # Mean 0.3
    prec   = rbeta(N, 10, 10*2 - 10) # Mean 0.5
  )
  
  theta[, hist(crate)]
  
  seeds <- sample.int(.Machine$integer.max, N, TRUE)
  
  matrices <- parallel::mclapply(1:N, FUN = function(i) {
    fn <- sprintf("calibration/simulated_data/sir-%06i.rds", i)
    
    if (file.exists(fn))
      return(readRDS(fn))
    
    set.seed(seeds[i])
    
    m <- theta[i,
               ModelSIRCONN(
                 "mycon",
                 prevalence        = preval,
                 contact_rate      = crate,
                 transmission_rate = ptran,
                 recovery_rate     = prec, 
                 n                 = n
               )
    ]
    
    # Avoids printing
    verbose_off(m)
    
    run(m, ndays = ndays)
    
    # Using prepare_data
    ans <- prepare_data(m)
    saveRDS(ans, fn)
    
    ans
  }, mc.cores = ncores)
  
  # Keeping only the non-null elements
  is_not_null <- intersect(
    which(!sapply(matrices, inherits, what = "error")),
    which(!sapply(matrices, function(x) any(is.na(x))))
  )
  matrices <- matrices[is_not_null]
  theta    <- theta[is_not_null,]
  
  N <- length(is_not_null)
  
  # Setting up the data for tensorflow
  arrays_1d <- array(dim = c(N, dim(matrices[[1]][1,,])))
  for (i in seq_along(matrices))
    arrays_1d[i,,] <- matrices[[i]][1,,]
  
  theta2 <- copy(theta)
  theta2[, crate := plogis(crate / 10)]
  
  # Saving the data 
  saveRDS(
    list(
      theta = theta2,
      simulations = arrays_1d
    ),
    file = savefile,
    compress = TRUE
  )
}
```

**Parameters**:

-   `N`: Number of simulations to run (default 20,000).

-   `n`: Population size for each simulation (default 5,000).

-   `ndays`: Number of days to run each simulation (default 50).

-   `ncores`: Number of CPU cores used for parallel processing (default 20).

-   `seed`: Random seed for reproducibility.

-   `savefile`: Path to save the final output.

# prepare training and testing data

This function prepares simulation data for training machine learning models. It splits the data into training and test sets based on the specified training fraction, reshapes the data, and returns it in a format ready for the CNN model.

```{r}
#| label: Function to prepare training and testing data
prepare_data_sets <- function(datafile = "calibration/sir.rds", train_fraction = 0.7) {
  sim_results <- readRDS(datafile)
  theta <- sim_results$theta
  arrays_1d <- sim_results$simulations
  
  # Extracting infections only
  arrays_1d <- arrays_1d[,1,,drop=FALSE]
  N     <- dim(arrays_1d)[1]
  
  # Reshaping
  N_train <- floor(N * train_fraction)
  id_train <- 1:N_train
  train <- list(
    x = array_reshape(
      arrays_1d[id_train,,], dim = c(N_train, dim(arrays_1d)[-1])
    ),
    y =  array_reshape(
      as.matrix(theta)[id_train,], dim = c(N_train, ncol(theta)))
  )
  
  N_test <- N - N_train
  id_test <- (N_train + 1):N
  
  test <- list(
    x = array_reshape(arrays_1d[id_test,,], dim = c(N_test, dim(arrays_1d)[-1])),
    y = array_reshape(as.matrix(theta)[id_test,], dim = c(N_test, ncol(theta)))
  )
  
  list(train = train, test = test, theta = theta, arrays_1d = arrays_1d, N = N, N_train = N_train, N_test = N_test)
}
```

# build and train the model

The `build_and_train_model` function builds, trains, and evaluates a convolutional neural network (CNN) model using the `keras3` and `tensorflow` R packages.

```{r}
#|label: Function to build and train the model
build_and_train_model <- function(train, test, arrays_1d, theta, N_train, seed = 331, save_model_file = "sir-keras_infections_only") {
  # Build the model
  model <- keras3::keras_model_sequential()
  model |>
    keras3::layer_conv_2d(
      filters     = 32,
      input_shape = c(dim(arrays_1d)[-1], 1),
      activation  = "linear",
      kernel_size = c(1, 5)
    ) |>
    keras3::layer_max_pooling_2d(
      pool_size = 2,
      padding = 'same'
    ) |>
    keras3::layer_flatten(
      input_shape = dim(arrays_1d)[-1]
    ) |>
    keras3::layer_dense(
      units = ncol(theta),
      activation = 'sigmoid'
    )
  
  # Compile the model
  model %>% compile(
    optimizer = 'adam',
    loss      = 'mse',
    metric    = 'accuracy'
  )
  
  # Running the model
  tensorflow::set_random_seed(seed)
  model |> fit(
    train$x,
    train$y,
    epochs = 50,
    verbose = 0
  )
  
  pred <- predict(model, x = test$x) |>
    as.data.table() |>
    setnames(colnames(theta))
  
  MAEs <- abs(pred - as.matrix(test$y)) |>
    colMeans() |>
    print()
  

  list(pred = pred, MAEs = MAEs)
}
```

### **Parameters**:

-   `train`: The training data, including inputs (`x`) and targets (`y`).

-   `test`: The test data for model evaluation.

-   `arrays_1d`: The 1D simulation data used to define the input shape for the model.

-   `theta`: The target variables from the simulations.

-   `N_train`: Number of training samples.

-   `seed`: A random seed for reproducibility (default: 331).

-   `save_model_file`: Filename to save the model (currently unused in the code).

# visualize results

The `visualize_results` function generates visualizations to compare predicted values from a trained model with the test dataset's observed (true) values.

```{r}
#|label: Function to visualize results
visualize_results <- function(pred, test, theta, MAEs, N, N_train, output_file = "calibration/sir_infections_only.png") {
  pred[, id := 1L:.N]
  pred[, crate := qlogis(crate)]
  pred_long <- melt(pred, id.vars = "id")
  
  theta_long <- test$y |> as.data.table()
  setnames(theta_long, names(theta))
  theta_long[, id := 1L:.N]
  theta_long[, crate := qlogis(crate)]
  theta_long <- melt(theta_long, id.vars = "id")
  
  alldat <- rbind(
    cbind(pred_long, Type = "Predicted"),
    cbind(theta_long, Type = "Observed")
  )
  
  p1<-ggplot(alldat, aes(x = value, colour = Type)) +
    facet_wrap(~variable, scales = "free") +
    geom_boxplot()
 print(p1) 
  alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
  
  vnames <- data.table(
    variable = c("preval", "crate", "ptran", "prec"),
    Name     = paste(
      c("Init. state", "Contact Rate", "P(transmit)", "P(recover)"),
      sprintf("(MAE: %.2f)", MAEs)
    )
  )
  
  alldat_wide <- merge(alldat_wide, vnames, by = "variable")
  
  p2<-ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
    facet_wrap(~ Name, scales = "free") +
    geom_abline(slope = 1, intercept = 0) +
    geom_point(alpha = .2) +
    labs(
      title    = "Observed vs Predicted (validation set)",
      subtitle = sprintf(
        "The model includes %i simulated datasets, of which %i were used for training.",
        N,
        N_train
      ),
      caption  = "Predictions made using a CNN(infections only) as implemented with loss function MAE."
    )
  print(p2)
  
 
}
```

# Running the Model

```{r}
#| label: Main execution function
main <- function() {
  # Simulate data
  simulate_data()
  
  # Prepare data sets
  data_sets <- prepare_data_sets()
  train <- data_sets$train
  test <- data_sets$test
  theta <- data_sets$theta
  arrays_1d <- data_sets$arrays_1d
  N <- data_sets$N
  N_train <- data_sets$N_train
  
  # Build and train the model
  model_results <- build_and_train_model(train, test, arrays_1d, theta, N_train)
  pred <- model_results$pred
  MAEs <- model_results$MAEs
  
  # Visualize results
  print(visualize_results(pred, test, theta, MAEs, N, N_train))
}

# Run the main function
main()
```

# Section 2

Now we can run a CNN model to find the best parameters we can use for our CNN model to perform

This build_and_train_model function is designed to create, compile, train, and evaluate a convolutional neural network (CNN) model using the keras3 library in R for deep learning tasks.

```{r}
build_and_train_model <- function(train, test, theta, seed,
                                  filters, kernel_size, activation_conv,
                                  activation_dense, pool_size, optimizer,
                                  loss, epochs, verbose = 0) {
  # Build the model
  model <- keras::keras_model_sequential()
  model %>%
    keras::layer_conv_2d(
      filters     = filters,
      input_shape = c(dim(train$x)[2], dim(train$x)[3], 1),
      activation  = activation_conv,
      kernel_size = kernel_size
    ) %>%
    keras::layer_max_pooling_2d(
      pool_size = pool_size,
      padding = 'same'
    ) %>%
    keras::layer_flatten() %>%
    keras::layer_dense(
      units = ncol(theta),
      activation = activation_dense
    )
  
  # Compile the model
  model %>% keras::compile(
    optimizer = optimizer,
    loss      = loss,
    metrics   = 'mae'
  )
  
  # Set random seed
  tensorflow::set_random_seed(seed)
  
  # Fit the model
  model %>% keras::fit(
    train$x,
    train$y,
    epochs = epochs,
    verbose = verbose
  )
  
  # Make predictions
  pred <- predict(model, x = test$x) %>%
    as.data.table() %>%
    setnames(colnames(theta))
  
  # Calculate MAEs
  MAEs <- abs(pred - test$y) %>%
    colMeans()
  
  # Return the MAEs and predictions
  list(pred = pred, MAEs = MAEs, model = model)
}
```

-   **Building the model**:

    -   The function begins by creating a sequential CNN model using `keras3::keras_model_sequential()`.

    -   The first layer is a 2D convolutional layer with customizable `filters`, `kernel_size`, and `activation_conv`, applied to input data of shape `(height, width, 1)` (grayscale images).

    -   A max-pooling layer follows, which reduces spatial dimensions using a pool size (`pool_size`).

    -   The model is then flattened to transition from 2D to 1D data.

    -   A dense (fully connected) layer is added with the number of units equal to the number of columns in `theta` and an activation function `activation_dense`.

-   **Compiling the model**:

    -   The model is compiled with an optimizer (`optimizer`), loss function (`loss`), and the metric mean absolute error (`mae`) to track performance.

        # visualize results

```{r}
#|label: Function to visualize results
visualize_results <- function(pred, test, theta, MAEs, N, N_train, output_file = NULL) {
  pred[, id := 1L:.N]
  pred_long <- melt(pred, id.vars = "id")
  
  theta_long <- as.data.table(test$y)
  setnames(theta_long, names(theta))
  theta_long[, id := 1L:.N]
  theta_long <- melt(theta_long, id.vars = "id")
  
  alldat <- rbind(
    cbind(pred_long, Type = "Predicted"),
    cbind(theta_long, Type = "Observed")
  )
  
  # Density plots
  p1 <- ggplot(alldat, aes(x = value, colour = Type)) +
    facet_wrap(~variable, scales = "free") +
    geom_density() +
    labs(title = "Density Plots of Predicted vs Observed Values",
         subtitle = "Comparing distributions of predicted and observed parameters",
         x = "Parameter Value", y = "Density")
  
  print(p1)
  
  # Scatter plots of Observed vs Predicted
  alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
  
  vnames <- data.table(
    variable = names(theta),
    Name     = paste(
      c("Initial Prevalence", "Contact Rate", "Transmission Probability", "Recovery Probability"),
      sprintf("(MAE: %.4f)", MAEs)
    )
  )
  
  alldat_wide <- merge(alldat_wide, vnames, by = "variable")
  
  p2 <- ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
    facet_wrap(~ Name, scales = "free") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
    geom_point(alpha = .2) +
    labs(
      title    = "Observed vs Predicted (Test Set)",
      subtitle = sprintf(
        "Best Model with Mean MAE: %.4f",
        mean(MAEs)
      ),
      x = "Observed Values",
      y = "Predicted Values"
    )
  
  print(p2)
}
```

# hyperparameter tuning

The `main` function orchestrates a full machine learning workflow for training a convolutional neural network (CNN) on simulated data.

```{r}
#|label: Main execution function with hyperparameter tuning
main <- function() {
  # Simulate data
  simulate_data()
  
  # Prepare data sets
  data_sets <- prepare_data_sets()
  train <- data_sets$train
  test <- data_sets$test
  theta <- data_sets$theta
  N <- data_sets$N
  N_train <- data_sets$N_train
  
  # Reshape the data for Keras
  train$x <- array(train$x, dim = c(dim(train$x)[1], dim(train$x)[2], dim(train$x)[3], 1))
  test$x <- array(test$x, dim = c(dim(test$x)[1], dim(test$x)[2], dim(test$x)[3], 1))
  
  # Define hyperparameter grid
  hyper_grid <- expand.grid(
    filters = c(16, 32, 64),
    kernel_size = list(c(1,3), c(1,5)),
    activation_conv = c('relu', 'linear'),
    activation_dense = c('sigmoid'),
    pool_size = list(c(1,2), c(1,3)),
    optimizer = c('adam'),
    loss = c('mse', 'mae'),
    epochs = c(50),
    stringsAsFactors = FALSE
  )
  
  # Initialize variables to store the best model
  best_MAE <- Inf
  best_model <- NULL
  best_pred <- NULL
  best_MAEs <- NULL
  best_params <- NULL
  
  # Loop over hyperparameter combinations
  for (i in 1:nrow(hyper_grid)) {
    cat("Testing model", i, "of", nrow(hyper_grid), "\n")
    
    # Extract hyperparameters
    filters <- hyper_grid$filters[i]
    kernel_size <- hyper_grid$kernel_size[[i]]
    activation_conv <- hyper_grid$activation_conv[i]
    activation_dense <- hyper_grid$activation_dense[i]
    pool_size <- hyper_grid$pool_size[[i]]
    optimizer <- hyper_grid$optimizer[i]
    loss <- hyper_grid$loss[i]
    epochs <- hyper_grid$epochs[i]
    
    # Set a seed for reproducibility
    seed <- 331
    
    # Build and train the model
    model_results <- tryCatch(
      {
        build_and_train_model(
          train = train,
          test = test,
          theta = theta,
          seed = seed,
          filters = filters,
          kernel_size = kernel_size,
          activation_conv = activation_conv,
          activation_dense = activation_dense,
          pool_size = pool_size,
          optimizer = optimizer,
          loss = loss,
          epochs = epochs,
          verbose = 0
        )
      },
      error = function(e) {
        cat("Error in model", i, ":", e$message, "\n")
        return(NULL)
      }
    )
    
    # If the model failed, skip to the next iteration
    if (is.null(model_results)) next
    
    # Get the MAEs
    MAEs <- model_results$MAEs
    
    # Store the average MAE
    hyper_grid$MAE[i] <- mean(MAEs)
    
    # If this is the best MAE so far, save the model and predictions
    if (hyper_grid$MAE[i] < best_MAE) {
      best_MAE <- hyper_grid$MAE[i]
      best_model <- model_results$model
      best_pred <- model_results$pred
      best_MAEs <- MAEs
      best_params <- hyper_grid[i,]
    }
  }
  
  # Print the best hyperparameters
  cat("Best model parameters:\n")
  print(best_params)
  cat("Best MAE:", best_MAE, "\n")
  
  # Visualize results
  visualize_results(best_pred, test, theta, best_MAEs, N, N_train)
}
```

### **1. Generate Parameters and Seeds**

-   The function starts by generating a set of parameters (`theta`) using `generate_theta`, which creates a matrix based on `N` and `n`.

-   It also generates random seeds (`seeds`) for each simulation using `sample.int()` to ensure reproducibility.

### 2. **Run Simulations**

-   Simulations are run using `run_simulations`, which likely generates a set of matrices representing the simulated data. The inputs include the number of simulations (`N`), days (`ndays`), and the number of cores (`ncores`).

### 3. **Filter Non-Null Data**

-   After simulations, non-null matrices are filtered out using `filter_non_null`, which removes invalid results and updates `matrices`, `theta`, and `N` accordingly.

### 4. **Prepare Data for TensorFlow**

-   The simulation data is then transformed to be used in TensorFlow via `prepare_data_for_tensorflow`, which likely reshapes the data into the format needed for model training.

### 5. **Data Adjustment and Saving**

-   The `theta` matrix is adjusted by converting the `crate` column using the logistic (sigmoid) function (`plogis(crate / 10)`).

-   There is an option to save the data (`theta` and the reshaped arrays) using `saveRDS`, though it's commented out.

### 6. **Split Data into Training and Testing Sets**

-   The data is split into training and testing sets using `split_data`, which returns `train`, `test`, and the number of training examples (`N_train`).

### 7. **Define Hyperparameter Grid**

-   A grid of hyperparameters is defined using `expand.grid()`, which includes options for:

    -   Number of filters in the convolutional layer (`filters`).

    -   Kernel sizes (`kernel_size`).

    -   Activation functions for the convolutional and dense layers (`activation_conv`, `activation_dense`).

    -   Pooling size (`pool_size`).

    -   Optimizer (`adam`).

    -   Loss functions (`mse`, `mae`).

    -   Number of epochs (`epochs`).

### 8. **Hyperparameter Tuning Loop**

-   A loop iterates through each combination of hyperparameters in `hyper_grid`.

-   For each combination, the function prints the current model number and extracts the respective hyperparameters.

-   A random seed (`seed = 331`) is set for reproducibility.

-   The function then calls `build_and_train_model` to build, compile, and train the CNN model.

-   If an error occurs during model training, it is caught using `tryCatch`, and the iteration moves to the next combination.

### 9. **Evaluate Models**

-   After training, the model's performance is evaluated using mean absolute errors (MAEs).

-   If the current model's average MAE is the best so far, the function stores the model, predictions, MAEs, and corresponding hyperparameters as the best model.

### 10. **Display Best Model**

-   After the hyperparameter tuning loop, the function prints the best hyperparameters and the lowest MAE achieved.

### 11. **Visualize Results**

-   Finally, the function calls `visualize_results` to display the predictions and compare them to the true values using the test set, `theta2`, and other relevant metrics.

# Results

```{r}
#| label: Run the main function
N <- 2e4   # Adjust N as needed
n <- 5000
ndays <- 50
ncores <- 20
main()

```
